# 代码理解

## 1. 项目理解

### **1.1 sampling_worker.py**

1.初始化旧策略与参考策略模型，均置为 `eval()` 并禁梯度，加载 tokenizer 和 GSM8K 数据迭代器。

2.`sample_batch` 先调用 `sample_trajectory` 生成 episode，再用 `get_batch_log_probs` 计算旧/参考 log prob，补齐 Episode，最后通过 ZeroMQ 发送。

---

重要超参：*每个 batch 里： len(episodes) = num_questions_per_batch * num_answers_per_question（4） = sample_batch_size（8）*

每次传给**`training_worker.py`** 8个*episodes（2个问题，每个问题4个答案）*

---

假设：

- 一批只有 **1 个问题**，但 `num_answers_per_question = 2`，所以会生成 2 条 episodes
- 问题是（GSM8K 风格）：

> Question: John had 3 apples. He bought 5 more. How many apples does he have now?
> 
- 标准答案（ `answer = "8"`）

episode样式为

```python
episode_1 = {
  'prefix'='<|system|>You are ...<|assistant|>',
  'prefix_tokens'= ['<|system|>', 'You', 'are', ... , '<|assistant|>'],
  'prefix_token_ids'=[1, 8532, 45, ..., 32001],
  generated_token_ids = [4001, 234, 9987, ..., 7] # 只包含模型生成的回答部分
	whole_token_ids = [pad, pad, ..., <前缀 50 个>, 4001, 234, ...] # 包含左 padding + prefix + 回答
  ['text'] = (
    "<|system|>You are ...<|assistant|>"
    "<think>We add 3 and 5 to get 8. So the answer is 8.</think><answer>8</answer>")
  ['reward'] = 1.25 + 1.0
  ['reward_info'] = {
    "format_reward": 1.25,   # 有正确的 <think> + <answer> 结构
    "answer_reward": 1.0, }   # 答案数字 8 正确
   'old_policy_log_probs':
   'ref_policy_log_probs':

)
}
prefix是提示词+问题
text是提示词+问题+回答

old_policy_log_probs / ref_policy_log_probs这两个字段是 长度 = 序列长度 - 1 的一维数组，对应每个 token 的 log_prob：
用 reward → 计算 advantage
用 old/ref 的 log_prob → 做 GRPO/PPO 风格的比值 / KL 等
```

---

1. `main()`→ 读 config，建 `SamplingWorker`，调用 `worker.run()`
2. `SamplingWorker.__init__`
    
    → 加载模型 & tokenizer、构造 `Gsm8kTasksDataset` 和 `DataLoader`、建 ZMQ PUSH socket
    
3. `run()` 循环：
    - `episodes = sample_batch()`
    - `serialized_episodes = serialize_episodes(episodes)`
    - `pickle.dumps` 后通过 ZMQ 发给训练进程
4. `sample_batch()`：
    - 从 dataloader 拿 `MiniBatch`（包含多个问题 + prefix + 标准答案）
    - 调 `sample_trajectory`：对每个问题 sample 多个回答 → 得到 `Episode` 列表
    - 再对每条 episode 的 whole_token_ids，用 old/ref 算 log_prob，写回 episode
5. `sample_trajectory()`：
    - 左 padding + 复制 prefix，构造大 batch
    - `model.generate` 生成最大 `max_gen_len` 的回答
    - 解码文本，调用 `reward_function` 算 reward & reward_info
    - 打包成 `Episode`（prefix / token_ids / 文本 / reward / reward_info / log_prob 占位）

### **1.2training_worker.py**

1.负责加载可训练的新策略模型，必要时套 LoRA，并调用 DeepSpeed 初始化（Gloo 后端、ZeRO Stage 2、CPU offload）。

2.主进程从 ZeroMQ PULL 数据后广播到所有 rank；各 rank 按问题数切分。

3.`train_step` 重新算新策略 log prob、按组计算 advantage，调用 GSPO 或 GRPO loss，并执行 `model_engine.backward/step`。

**2.2.1 调用 GSPO 或 GRPO loss展开**

两者都用到：

- `old_policy_log_probs`：采样时的旧策略；
- `new_policy_log_probs`：当前训练中的新策略；
- `ref_policy_log_probs`：固定不变的参考策略；
- `prefix_len`：只对“生成部分”做 PPO/GRPO 约束（裁掉 prompt 部分）。

**GSPO（gspo_loss）：**

- 按 **序列级别** 的 log prob 差值来算重要性比例：
    - 把 `prefix_len` 之前裁掉，只看生成段；
    - 计算每条序列的总 log prob 差（new - old），再除以有效长度，求指数得到 `importance_ratio`；
    - 再 clip 到 `[1 - ε, 1 + ε]` 得到 `cliped_ratio`；
    - 乘上 **序列级 advantage**，取 `min(importance_term, clip_term)`；
    - 再减去一个 KL 项（new 和 ref 的差），得到 `objective_function`；
    - `loss = - objective_function.mean()`。

**GRPO（grpo_loss）：**

- 更接近传统 PPO，按 **token 级** 的 log prob 差来算比例：
    - 对生成部分的每个 token 计算 `exp(new - old)` 得到 per-token ratio；
    - clip 后乘上 per-token advantages；
    - KL 同样是 new vs ref 的 per-token KL；
    - 通过 `attention_mask` 把 pad 掉，按长度平均，再对 batch 求平均。

两者本质都是：**在 advantage 加权的目标上加一个 KL 正则，并用 clipped importance ratio 控制 policy 更新幅度**，只不过一个是“按整个序列”，一个是“按每个 token”。

---

**串起来总结一下：**

1. `main()` 读 config，构建 `TrainingWorker`，初始化模型、DeepSpeed、ZMQ。
2. `run()` 里：
    - **只有 rank 0** 用 ZeroMQ 从采样进程 PULL 一批 `episodes`；
    - 做 batch 大小和 world_size 的整除检查；
    - 用 `broadcast_episodes` 把数据以 tensor 形式广播给所有 rank，并按照“题数 × 每题答案数”切成每个 rank 一份。
3. 每个 rank 对自己的那份 episodes 调 `train_step()`：
    - 用 `model_engine` 重算 **新策略** 对整条序列的 log prob；
    - 用 `group_advantages` 按题标准化 reward 得到 advantage；
    - 把采样时存的旧策略 / 参考策略 log prob 装成 tensor；
    - 调 `gspo_loss` 或 `grpo_loss` 算出这一批的损失。
4. 调 `model_engine.backward(loss)` + `model_engine.step()`，由 DeepSpeed 完成分布式梯度同步和参数更新。
5. rank 0 定期把最新参数写 checkpoint 给采样进程、顺便做 eval。